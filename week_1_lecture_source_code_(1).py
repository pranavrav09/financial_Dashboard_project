# -*- coding: utf-8 -*-
"""Week 1 Lecture Source Code (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QO4FWlpj6_5owqYtl1CpLRj7JiZV5Z5u

# Week 1 Lecture Source Code

## Setup and Loading Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import yfinance as yf
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import scipy.stats as stats
from datetime import datetime, timedelta
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split
from matplotlib.ticker import FuncFormatter

"""## Preliminary Visualizations

### Time Plots

Specifying stock ticker, start date, and end date. In our example, the stock is apple (Ticker: AAPL).
"""

TICKER = 'AAPL' # Stock Ticker symbol
START_DATE = '2013-06-01' # Stock data start date
END_DATE = '2023-06-01' # Stock data end date

# Store price details into a variable
stockPx = yf.download(TICKER, START_DATE, END_DATE)

"""Extracting adjusted close prices of Apple's stock from Yahoo Finance."""

stockPxAj = stockPx['Adj Close'] # index adjusted close price column
stockPxAj.plot(title = TICKER + ' Daily Prices', ylabel = 'Price in USD ($)', color = 'k')

"""Calculate the log returns."""

# Covert stock price to log returns
stockLogRet = np.log(stockPxAj).diff().dropna() # calculate log returns and drop null values
# Time plot of stock log returns
stockLogRet.plot(title = TICKER + ' Daily Log Returns', ylabel = 'Log Return', color = 'k')

"""### Scatter Plots

<p style="color:red;">Some values by the time you run this may be different than those in the lecture slides because the code performs visualizations and analyses from the rolling 5 years data (set via the 'TAU' variable), so everyday a new data point is observed and the oldest data point is removed.</p>

Specifying your asset tickers and their data duration
"""

TICKER1 = 'ADBE' # Enter the stock 1 ticker here
TICKER2 = 'CMG' # Enter the stock 2 ticker here
TAU = 5          # Enter duration of data you want from Yahoo Finance

"""Visualizing the relationship between ADBE and CMG log returns."""

START_DATE = (datetime.today() - timedelta(days = TAU * 365)).strftime('%Y-%m-%d') # data start date
END_DATE = datetime.today().strftime('%Y-%m-%d') # data end date

stockPxPair = yf.download([TICKER1, TICKER2], START_DATE, END_DATE)['Adj Close'] # retreiving asset price data from yahoo finance

# converting prices to log returns and removing NaN values
stockLogRetPair = np.log(stockPxPair).diff().dropna()
stockLogRetPair.plot.scatter(x=TICKER1, y=TICKER2)
plt.title('Scatter Plot ' + TICKER1 + ' vs ' + TICKER2)

"""### Correlation Heatmap

Specifying your asset tickers and their data duration
"""

symbolList = ['AAPL', 'ADBE', 'GLD', 'CMG', 'JPM', 'WFC'] # Enter the list of stock symbols here
# the duration of data was listed above via TAU

"""Visualizing the relationshipa between the listed stocks"""

START_DATE = (datetime.today() - timedelta(days = TAU * 365)).strftime('%Y-%m-%d') # data start date
END_DATE = datetime.today().strftime('%Y-%m-%d') # data end date

stockPxList = yf.download(symbolList, START_DATE, END_DATE)['Adj Close'] # retreiving asset price data from yahoo finance

# converting prices to log returns and removing NaN values
stockLogRetList = np.log(stockPxList).diff().dropna()
# visualizing correlation heatmap
fig, ax = plt.subplots(figsize=(15,10))
sns.heatmap(stockLogRetList.corr(),annot=True)
plt.title("Correlations Between Stock Log Returns")

"""### Stock Log Return Box Plot

Descriptive statistics to summarize/ aggregate log returns.
"""

stockLogRet.describe()

"""Plotting the box plot of Apple's log returns. Statistics like min, max, mean, range, interquartile range, and outliers can also be seen here."""

plt.boxplot(stockLogRet, vert=False) # stock log returns are extracted above when visualizing time plots
plt.title("Boxplot of "+ TICKER + "'s Stock Log Return")
plt.xlabel("Log Return")

"""### Histogram

Histogram is basically a box plot that shows frequencies/ probabilities.
"""

_, bins, _= plt.hist(stockLogRet, bins=50, density=1, alpha=0.5)
plt.title("Histogram of " + TICKER + "'s Stock Log Return")
plt.xlabel("Log Return")
plt.ylabel("Density")

"""## Preliminary Normality Testing

### Shapiro-Wilk Test
"""

shapiro_test = stats.shapiro(stockLogRet)
shapiro_test.pvalue

"""### Jarque-Bera Test

What can you say about the skewness and kurtosis of the blue line (Normal) vs the orange line (Student-t)?
"""

from scipy.stats import norm, t
mu = 0  # defining mean parameter
sigma = 1 # defining standard deviation parameter
# Notice that this is a Standard Normal Distribution
x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal pdf')

df = 2.74
mean, var, skew, kurt = t.stats(df, moments='mvsk')
x = np.linspace(t.ppf(0.01, df),t.ppf(0.99, df), 100)
plt.plot(x, t.pdf(x, df), label='Student-t pdf')
plt.title("Normal vs Student-t Distributions")
plt.xlabel("Values"); plt.ylabel("Density")
plt.legend()

"""<h3> Jarque-Bera Test (Experiment) </h3>"""

jarque_bera_test = stats.jarque_bera(stockLogRet)
jarque_bera_test.pvalue

"""## Data Pre-processing

### Economic Recession Data

We will use a small and simple dataset to demonstrate the data pre-processing stage of our analysis. We use the Federal Reserve’s Economic Data (FRED) service to download the US recession dates as defined by the National Bureau of Economic Research. We also source four variables that are commonly used to predict the onset of a recession (Kelley 2019) and available via FRED, namely:


<ul>
  <li>JHDUSRGDPBR: U.S. recession status</li>
  <li>T10Y3M: The long-term spread of the treasury yield curve, defined as the difference between the ten-year and the three-month Treasury yield</li>
  <li>UMCSENT: The University of Michigan’s consumer sentiment indicator</li>
  <li>NFCI: The National Financial Conditions Index (NFCI)</li>
  <li>NFCINONFINLEVERAGE: The NFCI nonfinancial leverage subindex</li>
  <li>GDPC1: Real GDP (inflation-adjusted)</li>
</ul>

We gather these data dating between 1980 and 2023, a total of 493 months of data.
"""

# Install pandas_datareader package if you haven't already
#! pip install pandas_datareader

import pandas_datareader as pdr

# specifying  economic indicators
INDICATORS = ['JHDUSRGDPBR', 'T10Y3M', 'NFCI', 'NFCINONFINLEVERAGE', 'UMCSENT', 'GDPC1']
# variable names
VAR_NAMES = ['recession', 'yield_curve', 'financial_conditions', 'leverage', 'sentiment', 'real_gdp']
# predictors for our models
FEATURES = VAR_NAMES[1:]
# what we are predicting - the recession status
RESPONSE = VAR_NAMES[0]
# variable display
VAR_DISPLAY = ['Recession', 'Yield Curve', 'Financial Conditions', 'Leverage', 'Sentiment', 'Real GDP']
col_dict = dict(zip(VAR_NAMES, VAR_DISPLAY))

# putting them all into one datafram'e
econ = (pdr.DataReader(INDICATORS, 'fred', 1980, 2023)
        .ffill()
        .resample('M')
        .last()
        .dropna())
econ.columns = VAR_NAMES

econ.head(10)

"""Since there are 493 observations and 493 of them are non-null values for each column. This means that there are no missing values for any of the variables."""

econ.info()

"""#### Remove missing values

You would run this code if there is determined to be missing values for any of the variables.
"""

econ = econ.dropna() # drop nan values from any of the rows

"""#### Standardizing features

We standardize the features so all indicators have mean 0 standard deviation of 1. We standardize the features mainly because they have different units of measure (e.g.,yield curve is in %, real GDP is in billions of dollars). We scale them so that they can have the same unit.
"""

econ.loc[:, FEATURES] = scale(econ.loc[:, FEATURES]) # standardizing indicators

econ.head(10)

"""#### Remove potential outliers

One way to spot potential outliers is to use the box plot on the selected feature. The example below shows potential outliers of the yield_curve feature.
"""

plt.boxplot(econ['financial_conditions'], vert=False) # stock log returns are extracted above when visualizing time plots
plt.title("Boxplot of the National Financial Condition")
plt.xlabel("Financial Condition Index")

"""Write a function so that for each of the feature, remove the values that lie more than 3 standard deviations away from the mean."""

# Function to remove outlying values that lie > 3 standard deviations away from the mean
def remove_outliers(df, columns, n_std):
    for col in columns:
        print('Working on column: {}'.format(col))

        mean = df[col].mean() # mean
        sd = df[col].std() # standard deviation

        df = df[(df[col] <= mean+(n_std*sd))] # criteria

    return df

econ1 = remove_outliers(econ, FEATURES, 3)
econ1.info()

"""#### Feature Importance

First, we have to split the data into 80% training set and 20% testing set.
"""

X_train, X_test, y_train, y_test = train_test_split(econ1.loc[:, FEATURES], econ1.loc[:, RESPONSE], test_size=0.2, random_state=0)

"""Since this is a classification problem (recession status 0 or 1), we use the Random Forest Classifier to find the importance score for each feature."""

from sklearn.ensemble import RandomForestClassifier # importing the random forest module

rf_model = RandomForestClassifier(random_state=0) # define the random forest model

rf_model.fit(X_train, y_train) # fit the random forest model

importances = rf_model.feature_importances_ # get importance

indices = np.argsort(importances) # sort the features' index by their importance scores

"""Ranking features by thier relative importance scores.<br>
Since all the features have importance scores >= 0.05, we do not drop any of them.
"""

plt.title('Feature Importances in the Random Forest Model')
plt.barh(range(len(indices)), importances[indices], align='center')
plt.yticks(range(len(indices)), [FEATURES[i] for i in indices])
plt.xlabel('Importance Score')

"""#### Remove multicollinearity

We can detect if an independent variable (feature) in a regression model is linearly correlated through a correlation heatmap.
<br><br>
Since none of the correlations are > 0.8 or < -0.8, we do not have to worry about dropping any feature columns due to multicollinearity.
"""

# plotting a correaltion heatmap to identify correlations b/w features
sns.heatmap(econ1.loc[:, FEATURES].corr(), annot=True)
plt.title("Correlations Between Recession Features")

"""Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model. In the case when there are too many features (e.g., >50) and the correlation heatmap looks complicated, VIF values can be used to identify highly correlated variables.<br><br>
A VIF of 1 indicates the features are not correlated, a VIF between 1 and 5 indicates moderate correlation, and a VIF above 5 indicates high correlation.<br><br>
We observe moderate correlations between the variables and conclude no particular features to be dropped for our analysis.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
pd.DataFrame({'Features': FEATURES, 'VIF': [variance_inflation_factor(econ1.loc[:, FEATURES].values, i) for i in range(len(FEATURES))]})

"""#### Label balancing

We count the number of months which the U.S. economy was and was not in a recession, then plotted them using a barplot.
The class distribution of recession status Matches about a 1:10 ratio or 429 (90..13%) observation in the majority class and 47 (9.87%) observation in the minority class. We need to rebalance this!
"""

ax = sns.countplot(x = "recession", data = econ1)
plt.title('Distribution of Recession Statuses')
plt.xlabel('Recession Statuses')
total = len(econ1["recession"])
for p in ax.patches:
        percentage = '{:.2f}%'.format(100 * p.get_height()/total)
        x_coord = p.get_x()
        y_coord = p.get_y() + p.get_height()+0.02
        ax.annotate(percentage, (x_coord, y_coord))

"""It is not necessary to apply the Synthetic Minority Over-sampling Technique (SMOTE) to rebalance the recession classes here as we are exploring our data as opposed to fitting a model to it. Despite this, I have comment-coded that demonstrates how SMOTE is implemented. More reference here: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/
<br><br>
Originally, SMOTE selects a minority class instance and finds its k (default: k=5) nearest minority class neighbors to upsample the minority class. Here, we make a little tweak to SMOTE to combine it with undersampling of the majority class. Specifically, we can update the recession data to first oversample the minority class to have 15 % the number of examples of the majority class, then use random undersampling to reduce the number of examples in the majority class to have 50 % more than the minority class.
"""

from collections import Counter
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from numpy import where

over = SMOTE(sampling_strategy=0.25)
under = RandomUnderSampler(sampling_strategy=0.6)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)
# transform the dataset
X_SMOTE, y_SMOTE = pipeline.fit_resample(econ1.loc[:, FEATURES], econ1.loc[:, RESPONSE])

# plotting label distribution after SMOTE
ax = sns.countplot(x='recession', data = pd.DataFrame(y_SMOTE))
plt.title('SMOTE on Recession Status Distribution')
plt.xlabel('Recession Statuses')
total = len(y_SMOTE)
for p in ax.patches:
        percentage = '{:.2f}%'.format(100 * p.get_height()/total)
        x_coord = p.get_x()
        y_coord = p.get_y() + p.get_height()+0.02
        ax.annotate(percentage, (x_coord, y_coord))

"""The dataset is transformed, first by oversampling the minority class, then undersampling the majority class.
Matches a 1:2 ratio or 297 (62.46%) observations in the majority class and 179 (37.54%) observations in the minority class.
Improved the minority-to-majority class ratio from 1:10 to 1:2, which seems to be a reasonable enough amount of bias for our analysis later.

##### Feature Value Distribution by Recession Status

We can also display box plots to show the feature distribution by recession status, recession vs no recession.
"""

# reshape the full data set into variable-value pairs
econ1_ = pd.melt(econ1.rename(columns=col_dict), id_vars='Recession').rename(columns=str.capitalize)
# display box plots to show the feature distribution by recession status
g = sns.catplot(x='Recession', y='Value', col='Variable', data=econ1_, kind='box')